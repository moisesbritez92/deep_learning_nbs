{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8865de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Configurar directorio de trabajo\n",
    "target_dir = os.getcwd() if 'cnn-cards' in os.getcwd().lower() else './CNN-Cards'\n",
    "\n",
    "if os.path.isdir(target_dir):\n",
    "    os.chdir(target_dir)\n",
    "print(f'Directorio actual: {os.getcwd()}')\n",
    "\n",
    "DATA_PATH = './Datasets/Cards/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01836e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Global variables\n",
    "SIZE = 224\n",
    "CLASSES = 53\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "path_models = 'Models'\n",
    "path_results = 'Results'\n",
    "\n",
    "print(f'TensorFlow version: {tf.__version__}')\n",
    "print(f'GPU disponible: {tf.config.list_physical_devices(\"GPU\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6cdffb",
   "metadata": {},
   "source": [
    "## Cargar datos de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079a071b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "test_dataset = test_generator.flow_from_directory(\n",
    "    DATA_PATH + 'test',\n",
    "    target_size=(SIZE, SIZE),\n",
    "    class_mode='categorical',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "class_names = list(test_dataset.class_indices.keys())\n",
    "print(f'Numero de clases: {len(class_names)}')\n",
    "print(f'Total imagenes test: {test_dataset.samples}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197c2dad",
   "metadata": {},
   "source": [
    "## Cargar modelos disponibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16905b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de modelos a cargar (en orden de preferencia: mejorado primero)\n",
    "model_candidates = {\n",
    "    'ViT': ['ViT_B16_3.h5', 'ViT_B16_2.h5'],\n",
    "    'MobileNet': ['MobileNet_3.h5', 'MobileNet_2.h5'],\n",
    "    'Custom': ['Custom_4.h5', 'Custom_3.h5']\n",
    "}\n",
    "\n",
    "models = {}\n",
    "model_names = []\n",
    "\n",
    "for model_type, candidates in model_candidates.items():\n",
    "    for candidate in candidates:\n",
    "        model_path = os.path.join(path_models, candidate)\n",
    "        if os.path.exists(model_path):\n",
    "            print(f'Cargando {candidate}...')\n",
    "            models[model_type] = tf.keras.models.load_model(model_path)\n",
    "            model_names.append(candidate.replace('.h5', ''))\n",
    "            break\n",
    "    else:\n",
    "        print(f'ADVERTENCIA: No se encontro ningun modelo para {model_type}')\n",
    "\n",
    "print(f'\\nModelos cargados: {list(models.keys())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3bdbc7",
   "metadata": {},
   "source": [
    "## Evaluar modelos individuales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b126d656",
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_accuracies = {}\n",
    "individual_predictions = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f'\\nEvaluando {model_name}...')\n",
    "    \n",
    "    # Evaluar\n",
    "    _, accuracy = model.evaluate(test_dataset, verbose=0)\n",
    "    individual_accuracies[model_name] = accuracy\n",
    "    \n",
    "    # Obtener predicciones (probabilidades)\n",
    "    test_dataset.reset()\n",
    "    predictions = model.predict(test_dataset, verbose=0)\n",
    "    individual_predictions[model_name] = predictions\n",
    "    \n",
    "    print(f'{model_name}: Accuracy = {accuracy:.4f}')\n",
    "\n",
    "# Resetear generador\n",
    "test_dataset.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1a9ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar accuracies individuales\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars = ax.bar(individual_accuracies.keys(), individual_accuracies.values(), color=['#3498db', '#e74c3c', '#2ecc71'])\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Accuracy Individual de Modelos')\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "for bar, acc in zip(bars, individual_accuracies.values()):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "            f'{acc:.4f}', ha='center', va='bottom', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(path_results, 'individual_accuracies.png'), dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0dfde4",
   "metadata": {},
   "source": [
    "## Ensemble: Soft Voting (promedio simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c00d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_voting(predictions_dict):\n",
    "    \"\"\"Promedio simple de probabilidades\"\"\"\n",
    "    preds_list = list(predictions_dict.values())\n",
    "    return np.mean(preds_list, axis=0)\n",
    "\n",
    "def weighted_voting(predictions_dict, weights_dict):\n",
    "    \"\"\"Promedio ponderado de probabilidades\"\"\"\n",
    "    weighted_preds = []\n",
    "    total_weight = sum(weights_dict.values())\n",
    "    \n",
    "    for model_name, preds in predictions_dict.items():\n",
    "        weight = weights_dict[model_name] / total_weight\n",
    "        weighted_preds.append(preds * weight)\n",
    "    \n",
    "    return np.sum(weighted_preds, axis=0)\n",
    "\n",
    "def calculate_accuracy(predictions, true_labels):\n",
    "    \"\"\"Calcular accuracy a partir de probabilidades\"\"\"\n",
    "    pred_classes = predictions.argmax(axis=1)\n",
    "    return np.mean(pred_classes == true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638e9c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener true labels\n",
    "true_labels = test_dataset.labels\n",
    "\n",
    "# Soft voting (promedio simple)\n",
    "ensemble_soft = soft_voting(individual_predictions)\n",
    "acc_soft = calculate_accuracy(ensemble_soft, true_labels)\n",
    "print(f'Ensemble (Soft Voting): Accuracy = {acc_soft:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49f8d3f",
   "metadata": {},
   "source": [
    "## Ensemble: Weighted Voting (ponderado por accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a413a4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar accuracy como pesos\n",
    "ensemble_weighted = weighted_voting(individual_predictions, individual_accuracies)\n",
    "acc_weighted = calculate_accuracy(ensemble_weighted, true_labels)\n",
    "print(f'Ensemble (Weighted Voting): Accuracy = {acc_weighted:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260d0b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizar pesos manualmente\n",
    "# Dar mas peso al mejor modelo\n",
    "best_model = max(individual_accuracies, key=individual_accuracies.get)\n",
    "optimized_weights = {name: 1.0 for name in models.keys()}\n",
    "optimized_weights[best_model] = 2.0  # Doble peso al mejor\n",
    "\n",
    "print(f'Pesos optimizados: {optimized_weights}')\n",
    "\n",
    "ensemble_optimized = weighted_voting(individual_predictions, optimized_weights)\n",
    "acc_optimized = calculate_accuracy(ensemble_optimized, true_labels)\n",
    "print(f'Ensemble (Optimized Voting): Accuracy = {acc_optimized:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b91263",
   "metadata": {},
   "source": [
    "## Comparacion de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77711be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumen de todos los resultados\n",
    "results = {\n",
    "    **{f'{name} (Individual)': acc for name, acc in individual_accuracies.items()},\n",
    "    'Ensemble (Soft Voting)': acc_soft,\n",
    "    'Ensemble (Weighted)': acc_weighted,\n",
    "    'Ensemble (Optimized)': acc_optimized\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'Modelo': results.keys(),\n",
    "    'Accuracy': results.values()\n",
    "}).sort_values('Accuracy', ascending=False)\n",
    "\n",
    "print('\\n=== RANKING DE MODELOS ===')\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad61f593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar comparacion\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "colors = ['#3498db'] * len(individual_accuracies) + ['#e74c3c', '#f39c12', '#27ae60']\n",
    "bars = ax.barh(results_df['Modelo'], results_df['Accuracy'], color=colors)\n",
    "\n",
    "ax.set_xlabel('Accuracy')\n",
    "ax.set_title('Comparacion: Modelos Individuales vs Ensemble')\n",
    "ax.set_xlim(0, 1)\n",
    "\n",
    "for bar, acc in zip(bars, results_df['Accuracy']):\n",
    "    ax.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,\n",
    "            f'{acc:.4f}', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(path_results, 'ensemble_comparison.png'), dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f72fb9",
   "metadata": {},
   "source": [
    "## Classification Report del mejor ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ed86b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar el mejor ensemble\n",
    "ensemble_results = {\n",
    "    'soft': acc_soft,\n",
    "    'weighted': acc_weighted,\n",
    "    'optimized': acc_optimized\n",
    "}\n",
    "best_ensemble_type = max(ensemble_results, key=ensemble_results.get)\n",
    "\n",
    "if best_ensemble_type == 'soft':\n",
    "    best_ensemble_preds = ensemble_soft\n",
    "elif best_ensemble_type == 'weighted':\n",
    "    best_ensemble_preds = ensemble_weighted\n",
    "else:\n",
    "    best_ensemble_preds = ensemble_optimized\n",
    "\n",
    "print(f'Mejor ensemble: {best_ensemble_type} con accuracy {ensemble_results[best_ensemble_type]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88099f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "pred_classes = best_ensemble_preds.argmax(axis=1)\n",
    "true_text = [class_names[i] for i in true_labels]\n",
    "pred_text = [class_names[i] for i in pred_classes]\n",
    "\n",
    "print('\\n=== Classification Report (Ensemble) ===')\n",
    "print(classification_report(true_text, pred_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b86f827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cf = confusion_matrix(true_labels, pred_classes)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "sns.heatmap(cf, annot=False, square=True, cbar=True,\n",
    "            cmap=plt.cm.Blues, xticklabels=class_names, yticklabels=class_names, ax=ax)\n",
    "ax.set_ylabel('Actual')\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_title(f'Confusion Matrix - Ensemble ({best_ensemble_type})')\n",
    "plt.xticks(rotation=90, fontsize=6)\n",
    "plt.yticks(fontsize=6)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(path_results, 'ensemble_confusion_matrix.png'), dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23552ca2",
   "metadata": {},
   "source": [
    "## Analisis de errores: Donde falla el ensemble?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d744d38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar clases mas confundidas\n",
    "errors = []\n",
    "for i, (true, pred) in enumerate(zip(true_labels, pred_classes)):\n",
    "    if true != pred:\n",
    "        errors.append({\n",
    "            'index': i,\n",
    "            'true_class': class_names[true],\n",
    "            'pred_class': class_names[pred],\n",
    "            'confidence': best_ensemble_preds[i, pred]\n",
    "        })\n",
    "\n",
    "errors_df = pd.DataFrame(errors)\n",
    "print(f'Total errores: {len(errors_df)} de {len(true_labels)} ({100*len(errors_df)/len(true_labels):.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f379f1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 confusiones mas comunes\n",
    "if len(errors_df) > 0:\n",
    "    confusion_pairs = errors_df.groupby(['true_class', 'pred_class']).size().reset_index(name='count')\n",
    "    confusion_pairs = confusion_pairs.sort_values('count', ascending=False).head(10)\n",
    "    \n",
    "    print('\\nTop 10 confusiones mas comunes:')\n",
    "    for _, row in confusion_pairs.iterrows():\n",
    "        print(f\"  {row['true_class']} -> {row['pred_class']}: {row['count']} veces\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f4cc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analisis de confianza en errores\n",
    "if len(errors_df) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.hist(errors_df['confidence'], bins=20, edgecolor='black', alpha=0.7)\n",
    "    ax.set_xlabel('Confianza de prediccion incorrecta')\n",
    "    ax.set_ylabel('Frecuencia')\n",
    "    ax.set_title('Distribucion de confianza en predicciones incorrectas')\n",
    "    ax.axvline(errors_df['confidence'].mean(), color='red', linestyle='--', \n",
    "               label=f'Media: {errors_df[\"confidence\"].mean():.3f}')\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(path_results, 'error_confidence_distribution.png'), dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6179cd",
   "metadata": {},
   "source": [
    "## Guardar funcion de ensemble para inferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96269b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar configuracion del ensemble\n",
    "ensemble_config = {\n",
    "    'models': list(models.keys()),\n",
    "    'model_files': [f'{name}.h5' for name in model_names],\n",
    "    'individual_accuracies': individual_accuracies,\n",
    "    'weights': optimized_weights if acc_optimized == max(ensemble_results.values()) else individual_accuracies,\n",
    "    'ensemble_type': best_ensemble_type,\n",
    "    'ensemble_accuracy': ensemble_results[best_ensemble_type]\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(os.path.join(path_models, 'ensemble_config.json'), 'w') as f:\n",
    "    json.dump(ensemble_config, f, indent=2)\n",
    "\n",
    "print('Configuracion del ensemble guardada en Models/ensemble_config.json')\n",
    "print(json.dumps(ensemble_config, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c39d885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumen final\n",
    "print('\\n' + '='*60)\n",
    "print('RESUMEN FINAL')\n",
    "print('='*60)\n",
    "\n",
    "best_individual = max(individual_accuracies.items(), key=lambda x: x[1])\n",
    "best_ensemble = max(ensemble_results.items(), key=lambda x: x[1])\n",
    "\n",
    "print(f'\\nMejor modelo individual: {best_individual[0]} ({best_individual[1]:.4f})')\n",
    "print(f'Mejor ensemble: {best_ensemble[0]} ({best_ensemble[1]:.4f})')\n",
    "\n",
    "improvement = (best_ensemble[1] - best_individual[1]) * 100\n",
    "if improvement > 0:\n",
    "    print(f'\\nMejora del ensemble sobre mejor individual: +{improvement:.2f}%')\n",
    "else:\n",
    "    print(f'\\nEl mejor modelo individual supera al ensemble por: {-improvement:.2f}%')\n",
    "\n",
    "print('='*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
