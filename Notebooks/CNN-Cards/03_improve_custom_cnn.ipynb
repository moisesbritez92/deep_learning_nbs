{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa51ef4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Configurar directorio de trabajo\n",
    "target_dir = os.getcwd() if 'cnn-cards' in os.getcwd().lower() else './CNN-Cards'\n",
    "\n",
    "if os.path.isdir(target_dir):\n",
    "    os.chdir(target_dir)\n",
    "print(f'Directorio actual: {os.getcwd()}')\n",
    "\n",
    "DATA_PATH = './Datasets/Cards/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d8af49",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv2D, MaxPooling2D, BatchNormalization, \n",
    "    Dropout, Dense, GlobalAveragePooling2D, Add, Activation\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Global variables\n",
    "SIZE = 224\n",
    "CLASSES = 53\n",
    "EPOCHS = 100\n",
    "PATIENCE_ES = 25\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "path_models = 'Models'\n",
    "path_results = 'Results'\n",
    "\n",
    "print(f'TensorFlow version: {tf.__version__}')\n",
    "print(f'GPU disponible: {tf.config.list_physical_devices(\"GPU\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eeda1af",
   "metadata": {},
   "source": [
    "## Cargar datos con Data Augmentation agresiva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6519db46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation agresiva para modelo custom\n",
    "train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1.0/255,\n",
    "    rotation_range=15,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    zoom_range=0.15,\n",
    "    fill_mode='reflect',\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    brightness_range=(0.9, 1.1),\n",
    "    channel_shift_range=20\n",
    ")\n",
    "\n",
    "valid_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.0/255)\n",
    "test_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "train_path = DATA_PATH + 'train'\n",
    "valid_path = DATA_PATH + 'valid'\n",
    "test_path = DATA_PATH + 'test'\n",
    "\n",
    "train_dataset = train_generator.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size=(SIZE, SIZE),\n",
    "    class_mode='categorical',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "valid_dataset = valid_generator.flow_from_directory(\n",
    "    valid_path,\n",
    "    target_size=(SIZE, SIZE),\n",
    "    class_mode='categorical',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_dataset = test_generator.flow_from_directory(\n",
    "    test_path,\n",
    "    target_size=(SIZE, SIZE),\n",
    "    class_mode='categorical',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c00e7f",
   "metadata": {},
   "source": [
    "## Evaluar modelo original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e892c3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar modelo original\n",
    "custom_original = tf.keras.models.load_model('Models/Custom_3.h5')\n",
    "print('Arquitectura original:')\n",
    "custom_original.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da76299c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, acc_original = custom_original.evaluate(test_dataset, verbose=0)\n",
    "print(f'Accuracy original (Custom_3): {acc_original:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1e628e",
   "metadata": {},
   "source": [
    "## Construir CNN mejorada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff8c499",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(x, filters, kernel_size=3, dropout_rate=0.25):\n",
    "    \"\"\"Bloque convolucional con BatchNorm y Dropout\"\"\"\n",
    "    x = Conv2D(filters, kernel_size, padding='same', kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(filters, kernel_size, padding='same', kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "def build_improved_cnn(input_shape=(224, 224, 3), num_classes=53):\n",
    "    \"\"\"Arquitectura CNN mejorada con 5 bloques convolucionales\"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Bloque 1: 32 filtros\n",
    "    x = conv_block(inputs, 32, dropout_rate=0.1)\n",
    "    \n",
    "    # Bloque 2: 64 filtros\n",
    "    x = conv_block(x, 64, dropout_rate=0.2)\n",
    "    \n",
    "    # Bloque 3: 128 filtros\n",
    "    x = conv_block(x, 128, dropout_rate=0.25)\n",
    "    \n",
    "    # Bloque 4: 256 filtros\n",
    "    x = conv_block(x, 256, dropout_rate=0.3)\n",
    "    \n",
    "    # Bloque 5: 512 filtros\n",
    "    x = conv_block(x, 512, dropout_rate=0.4)\n",
    "    \n",
    "    # Head de clasificacion\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(256, activation='relu', kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "custom_improved = build_improved_cnn()\n",
    "custom_improved.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d928e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar parametros\n",
    "trainable_params = np.sum([np.prod(v.shape) for v in custom_improved.trainable_variables])\n",
    "non_trainable_params = np.sum([np.prod(v.shape) for v in custom_improved.non_trainable_variables])\n",
    "print(f'Parametros entrenables: {trainable_params:,}')\n",
    "print(f'Parametros no entrenables: {non_trainable_params:,}')\n",
    "print(f'Total: {trainable_params + non_trainable_params:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943e8700",
   "metadata": {},
   "source": [
    "## Configurar entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e05a717",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'Custom_4'\n",
    "\n",
    "# Callbacks\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    os.path.join(path_models, name + '.h5'),\n",
    "    monitor='val_accuracy',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    min_delta=0.001,\n",
    "    patience=PATIENCE_ES,\n",
    "    verbose=1,\n",
    "    mode='max',\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=f'logs/{name}',\n",
    "    histogram_freq=1\n",
    ")\n",
    "\n",
    "callbacks_list = [checkpoint, early_stop, reduce_lr, tensorboard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e0046d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilar\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "custom_improved.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8052ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar\n",
    "history = custom_improved.fit(\n",
    "    train_dataset,\n",
    "    validation_data=valid_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks_list\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43bc31c",
   "metadata": {},
   "source": [
    "## Visualizar resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b208c5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_save(h, dir, name):\n",
    "    history_df = pd.DataFrame(h.history)\n",
    "    history_df['epoch'] = list(range(len(history_df)))\n",
    "    history_df.to_csv(os.path.join(dir, name + '.csv'), header=True, index=False)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0].plot(history_df['epoch'], history_df['loss'], label='Train Loss')\n",
    "    axes[0].plot(history_df['epoch'], history_df['val_loss'], label='Val Loss')\n",
    "    axes[0].set_title('Loss')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[1].plot(history_df['epoch'], history_df['accuracy'], label='Train Accuracy')\n",
    "    axes[1].plot(history_df['epoch'], history_df['val_accuracy'], label='Val Accuracy')\n",
    "    axes[1].set_title('Accuracy')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(dir, name + '_curves.png'), dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "plot_and_save(history, path_results, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939ffe5c",
   "metadata": {},
   "source": [
    "## Evaluacion en Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa7c8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar mejor modelo\n",
    "best_model = tf.keras.models.load_model(os.path.join(path_models, name + '.h5'))\n",
    "\n",
    "# Evaluar\n",
    "_, acc_improved = best_model.evaluate(test_dataset)\n",
    "\n",
    "print(f'\\n=== Comparacion de resultados ===')\n",
    "print(f'Accuracy original (Custom_3): {acc_original:.4f}')\n",
    "print(f'Accuracy mejorado (Custom_4): {acc_improved:.4f}')\n",
    "print(f'Mejora: {(acc_improved - acc_original)*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9078f9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "def show_report(model, dataframe):\n",
    "    labels = dataframe.class_indices\n",
    "    true_labels = dataframe.labels\n",
    "    pred_labels = model.predict(dataframe, verbose=0).argmax(axis=1)\n",
    "    keys_array = np.array(list(labels.keys()))\n",
    "    true_text = [keys_array[value] for value in true_labels]\n",
    "    pred_text = [keys_array[value] for value in pred_labels]\n",
    "    print(classification_report(true_text, pred_text))\n",
    "\n",
    "def show_matrix(model, dataframe):\n",
    "    labels = dataframe.class_indices\n",
    "    true_labels = dataframe.labels\n",
    "    pred_labels = model.predict(dataframe, verbose=0).argmax(axis=1)\n",
    "    keys_array = np.array(list(labels.keys()))\n",
    "    true_text = [keys_array[value] for value in true_labels]\n",
    "    pred_text = [keys_array[value] for value in pred_labels]\n",
    "    cf = confusion_matrix(true_text, pred_text, labels=keys_array)\n",
    "    fig, ax = plt.subplots(figsize=(14, 14))\n",
    "    sns.heatmap(cf, annot=False, square=True, cbar=True,\n",
    "                cmap=plt.cm.Blues, xticklabels=keys_array, yticklabels=keys_array, ax=ax)\n",
    "    ax.set_ylabel('Actual')\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_title(f'Confusion Matrix - {name}')\n",
    "    plt.xticks(rotation=90, fontsize=6)\n",
    "    plt.yticks(fontsize=6)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_report(best_model, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14210be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_matrix(best_model, test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8701f5b0",
   "metadata": {},
   "source": [
    "## Alternativa: CNN con ResNet-style skip connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f9f0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(x, filters):\n",
    "    \"\"\"Bloque residual simplificado\"\"\"\n",
    "    shortcut = x\n",
    "    \n",
    "    x = Conv2D(filters, 3, padding='same', kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = Conv2D(filters, 3, padding='same', kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    # Ajustar dimensiones si es necesario\n",
    "    if shortcut.shape[-1] != filters:\n",
    "        shortcut = Conv2D(filters, 1, padding='same')(shortcut)\n",
    "        shortcut = BatchNormalization()(shortcut)\n",
    "    \n",
    "    x = Add()([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def build_resnet_style_cnn(input_shape=(224, 224, 3), num_classes=53):\n",
    "    \"\"\"CNN con conexiones residuales\"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Stem\n",
    "    x = Conv2D(32, 7, strides=2, padding='same', kernel_regularizer=regularizers.l2(1e-4))(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(3, strides=2, padding='same')(x)\n",
    "    \n",
    "    # Bloques residuales\n",
    "    x = residual_block(x, 64)\n",
    "    x = MaxPooling2D(2)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    x = residual_block(x, 128)\n",
    "    x = MaxPooling2D(2)(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    \n",
    "    x = residual_block(x, 256)\n",
    "    x = MaxPooling2D(2)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = residual_block(x, 512)\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    # Head\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(256, activation='relu', kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    return Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Descomentar para entrenar version con skip connections\n",
    "# custom_resnet = build_resnet_style_cnn()\n",
    "# custom_resnet.summary()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
