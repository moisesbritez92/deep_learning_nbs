{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7637db7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Configurar directorio de trabajo\n",
    "target_dir = os.getcwd() if 'cnn-cards' in os.getcwd().lower() else './CNN-Cards'\n",
    "\n",
    "if os.path.isdir(target_dir):\n",
    "    os.chdir(target_dir)\n",
    "print(f'Directorio actual: {os.getcwd()}')\n",
    "\n",
    "DATA_PATH = './Datasets/Cards/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37ea0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, Dropout, GlobalAveragePooling2D, BatchNormalization\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Global variables\n",
    "SIZE = 224\n",
    "NUM_CLASSES = 13  # A, 2-10, J, Q, K\n",
    "SUIT_CLASSES = 4  # clubs, diamonds, hearts, spades\n",
    "EPOCHS = 100\n",
    "PATIENCE_ES = 25\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "path_models = 'Models'\n",
    "path_results = 'Results'\n",
    "\n",
    "print(f'TensorFlow version: {tf.__version__}')\n",
    "print(f'GPU disponible: {tf.config.list_physical_devices(\"GPU\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0605fc21",
   "metadata": {},
   "source": [
    "## Crear generador de datos personalizado\n",
    "\n",
    "Parseamos las etiquetas del nombre de las carpetas (ej: \"ace of clubs\" -> num=ace, suit=clubs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d32697e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapeo de nombres a indices\n",
    "NUM_MAP = {\n",
    "    'ace': 0, 'two': 1, 'three': 2, 'four': 3, 'five': 4,\n",
    "    'six': 5, 'seven': 6, 'eight': 7, 'nine': 8, 'ten': 9,\n",
    "    'jack': 10, 'queen': 11, 'king': 12\n",
    "}\n",
    "\n",
    "SUIT_MAP = {\n",
    "    'clubs': 0, 'diamonds': 1, 'hearts': 2, 'spades': 3\n",
    "}\n",
    "\n",
    "# Inversos para visualizacion\n",
    "NUM_NAMES = {v: k for k, v in NUM_MAP.items()}\n",
    "SUIT_NAMES = {v: k for k, v in SUIT_MAP.items()}\n",
    "\n",
    "def parse_card_label(folder_name):\n",
    "    \"\"\"Extrae numero y palo del nombre de la carpeta\"\"\"\n",
    "    # Formato: \"ace of clubs\", \"two of hearts\", etc.\n",
    "    # Joker no tiene palo - lo excluimos\n",
    "    if 'joker' in folder_name.lower():\n",
    "        return None, None\n",
    "    \n",
    "    parts = folder_name.lower().split(' of ')\n",
    "    if len(parts) != 2:\n",
    "        return None, None\n",
    "    \n",
    "    num_name, suit_name = parts\n",
    "    num_idx = NUM_MAP.get(num_name)\n",
    "    suit_idx = SUIT_MAP.get(suit_name)\n",
    "    \n",
    "    return num_idx, suit_idx\n",
    "\n",
    "# Test\n",
    "print(parse_card_label('ace of clubs'))  # (0, 0)\n",
    "print(parse_card_label('king of hearts'))  # (12, 2)\n",
    "print(parse_card_label('joker'))  # (None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f134cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "class MultiOutputDataGenerator(Sequence):\n",
    "    \"\"\"Generador de datos para modelo multi-output\"\"\"\n",
    "    \n",
    "    def __init__(self, directory, batch_size=32, target_size=(224, 224), \n",
    "                 shuffle=True, augment=False):\n",
    "        self.directory = directory\n",
    "        self.batch_size = batch_size\n",
    "        self.target_size = target_size\n",
    "        self.shuffle = shuffle\n",
    "        self.augment = augment\n",
    "        \n",
    "        # Recopilar todos los archivos de imagen\n",
    "        self.samples = []\n",
    "        for folder in Path(directory).iterdir():\n",
    "            if folder.is_dir():\n",
    "                num_idx, suit_idx = parse_card_label(folder.name)\n",
    "                if num_idx is not None and suit_idx is not None:\n",
    "                    for img_path in folder.glob('*.jpg'):\n",
    "                        self.samples.append((str(img_path), num_idx, suit_idx))\n",
    "                    for img_path in folder.glob('*.png'):\n",
    "                        self.samples.append((str(img_path), num_idx, suit_idx))\n",
    "        \n",
    "        print(f'Cargadas {len(self.samples)} imagenes de {directory}')\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples) // self.batch_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_samples = self.samples[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        \n",
    "        X = np.zeros((len(batch_samples), *self.target_size, 3), dtype=np.float32)\n",
    "        y_num = np.zeros((len(batch_samples), NUM_CLASSES), dtype=np.float32)\n",
    "        y_suit = np.zeros((len(batch_samples), SUIT_CLASSES), dtype=np.float32)\n",
    "        \n",
    "        for i, (img_path, num_idx, suit_idx) in enumerate(batch_samples):\n",
    "            img = load_img(img_path, target_size=self.target_size)\n",
    "            img_array = img_to_array(img) / 255.0\n",
    "            \n",
    "            if self.augment:\n",
    "                img_array = self._augment(img_array)\n",
    "            \n",
    "            X[i] = img_array\n",
    "            y_num[i, num_idx] = 1.0\n",
    "            y_suit[i, suit_idx] = 1.0\n",
    "        \n",
    "        return X, {'num_output': y_num, 'suit_output': y_suit}\n",
    "    \n",
    "    def _augment(self, img):\n",
    "        \"\"\"Aplicar augmentacion basica\"\"\"\n",
    "        # Random flip horizontal\n",
    "        if np.random.random() > 0.5:\n",
    "            img = np.fliplr(img)\n",
    "        # Random flip vertical\n",
    "        if np.random.random() > 0.5:\n",
    "            img = np.flipud(img)\n",
    "        # Random brightness\n",
    "        if np.random.random() > 0.5:\n",
    "            factor = np.random.uniform(0.9, 1.1)\n",
    "            img = np.clip(img * factor, 0, 1)\n",
    "        return img\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c589ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear generadores\n",
    "train_gen = MultiOutputDataGenerator(\n",
    "    DATA_PATH + 'train',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    target_size=(SIZE, SIZE),\n",
    "    shuffle=True,\n",
    "    augment=True\n",
    ")\n",
    "\n",
    "valid_gen = MultiOutputDataGenerator(\n",
    "    DATA_PATH + 'valid',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    target_size=(SIZE, SIZE),\n",
    "    shuffle=False,\n",
    "    augment=False\n",
    ")\n",
    "\n",
    "test_gen = MultiOutputDataGenerator(\n",
    "    DATA_PATH + 'test',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    target_size=(SIZE, SIZE),\n",
    "    shuffle=False,\n",
    "    augment=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3ce9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar un batch\n",
    "X_sample, y_sample = train_gen[0]\n",
    "print(f'Shape X: {X_sample.shape}')\n",
    "print(f'Shape y_num: {y_sample[\"num_output\"].shape}')\n",
    "print(f'Shape y_suit: {y_sample[\"suit_output\"].shape}')\n",
    "\n",
    "# Visualizar una muestra\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "for i in range(10):\n",
    "    ax = axes[i//5, i%5]\n",
    "    ax.imshow(X_sample[i])\n",
    "    num_idx = y_sample['num_output'][i].argmax()\n",
    "    suit_idx = y_sample['suit_output'][i].argmax()\n",
    "    ax.set_title(f'{NUM_NAMES[num_idx]} of {SUIT_NAMES[suit_idx]}')\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c804bc2",
   "metadata": {},
   "source": [
    "## Construir modelo Multi-Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e4d6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_multioutput_model(input_shape=(224, 224, 3), num_classes=13, suit_classes=4):\n",
    "    \"\"\"Modelo multi-output con backbone MobileNetV2\"\"\"\n",
    "    \n",
    "    # Backbone compartido\n",
    "    base_model = tf.keras.applications.MobileNetV2(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=input_shape\n",
    "    )\n",
    "    \n",
    "    # Descongelar ultimas capas\n",
    "    for layer in base_model.layers[:-30]:\n",
    "        layer.trainable = False\n",
    "    for layer in base_model.layers[-30:]:\n",
    "        layer.trainable = True\n",
    "    \n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = base_model(inputs)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    # Shared hidden layers\n",
    "    shared = Dense(256, activation='relu', kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    shared = Dropout(0.4)(shared)\n",
    "    shared = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(1e-4))(shared)\n",
    "    shared = Dropout(0.3)(shared)\n",
    "    \n",
    "    # Cabeza para Numero (13 clases)\n",
    "    num_branch = Dense(64, activation='relu', kernel_regularizer=regularizers.l2(1e-4))(shared)\n",
    "    num_branch = Dropout(0.2)(num_branch)\n",
    "    num_output = Dense(num_classes, activation='softmax', name='num_output')(num_branch)\n",
    "    \n",
    "    # Cabeza para Palo (4 clases)\n",
    "    suit_branch = Dense(32, activation='relu', kernel_regularizer=regularizers.l2(1e-4))(shared)\n",
    "    suit_branch = Dropout(0.2)(suit_branch)\n",
    "    suit_output = Dense(suit_classes, activation='softmax', name='suit_output')(suit_branch)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=[num_output, suit_output])\n",
    "    return model\n",
    "\n",
    "multioutput_model = build_multioutput_model()\n",
    "multioutput_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f06030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar arquitectura\n",
    "tf.keras.utils.plot_model(\n",
    "    multioutput_model, \n",
    "    to_file='Results/multioutput_architecture.png',\n",
    "    show_shapes=True,\n",
    "    show_layer_names=True,\n",
    "    dpi=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521f74d9",
   "metadata": {},
   "source": [
    "## Configurar y entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6186357d",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'MultiOutput_NumSuit'\n",
    "\n",
    "# Callbacks\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    os.path.join(path_models, name + '.h5'),\n",
    "    monitor='val_loss',  # Monitoreamos loss total\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.001,\n",
    "    patience=PATIENCE_ES,\n",
    "    verbose=1,\n",
    "    mode='min',\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=f'logs/{name}',\n",
    "    histogram_freq=1\n",
    ")\n",
    "\n",
    "callbacks_list = [checkpoint, early_stop, reduce_lr, tensorboard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d8349a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilar con pesos para cada salida\n",
    "multioutput_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss={\n",
    "        'num_output': 'categorical_crossentropy',\n",
    "        'suit_output': 'categorical_crossentropy'\n",
    "    },\n",
    "    loss_weights={\n",
    "        'num_output': 1.0,  # Numero mas importante (13 clases)\n",
    "        'suit_output': 0.5  # Palo menos clases, menor peso\n",
    "    },\n",
    "    metrics={\n",
    "        'num_output': 'accuracy',\n",
    "        'suit_output': 'accuracy'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f26b732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar\n",
    "history = multioutput_model.fit(\n",
    "    train_gen,\n",
    "    validation_data=valid_gen,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks_list\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086bc63a",
   "metadata": {},
   "source": [
    "## Visualizar resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319f1c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multioutput_history(h, dir, name):\n",
    "    history_df = pd.DataFrame(h.history)\n",
    "    history_df['epoch'] = list(range(len(history_df)))\n",
    "    history_df.to_csv(os.path.join(dir, name + '.csv'), header=True, index=False)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Loss total\n",
    "    axes[0, 0].plot(history_df['epoch'], history_df['loss'], label='Train')\n",
    "    axes[0, 0].plot(history_df['epoch'], history_df['val_loss'], label='Val')\n",
    "    axes[0, 0].set_title('Total Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy Numero\n",
    "    axes[0, 1].plot(history_df['epoch'], history_df['num_output_accuracy'], label='Train')\n",
    "    axes[0, 1].plot(history_df['epoch'], history_df['val_num_output_accuracy'], label='Val')\n",
    "    axes[0, 1].set_title('Number Accuracy')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy Palo\n",
    "    axes[1, 0].plot(history_df['epoch'], history_df['suit_output_accuracy'], label='Train')\n",
    "    axes[1, 0].plot(history_df['epoch'], history_df['val_suit_output_accuracy'], label='Val')\n",
    "    axes[1, 0].set_title('Suit Accuracy')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Loss por salida\n",
    "    axes[1, 1].plot(history_df['epoch'], history_df['num_output_loss'], label='Num Train')\n",
    "    axes[1, 1].plot(history_df['epoch'], history_df['val_num_output_loss'], label='Num Val')\n",
    "    axes[1, 1].plot(history_df['epoch'], history_df['suit_output_loss'], label='Suit Train')\n",
    "    axes[1, 1].plot(history_df['epoch'], history_df['val_suit_output_loss'], label='Suit Val')\n",
    "    axes[1, 1].set_title('Loss por Salida')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(dir, name + '_curves.png'), dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "plot_multioutput_history(history, path_results, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5677c0",
   "metadata": {},
   "source": [
    "## Evaluacion en Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2131c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar mejor modelo\n",
    "best_model = tf.keras.models.load_model(os.path.join(path_models, name + '.h5'))\n",
    "\n",
    "# Evaluar\n",
    "results = best_model.evaluate(test_gen)\n",
    "print(f'\\n=== Resultados en Test Set ===')\n",
    "print(f'Loss total: {results[0]:.4f}')\n",
    "print(f'Num Loss: {results[1]:.4f}')\n",
    "print(f'Suit Loss: {results[2]:.4f}')\n",
    "print(f'Num Accuracy: {results[3]:.4f}')\n",
    "print(f'Suit Accuracy: {results[4]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598ff5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy combinada (carta correcta = numero Y palo correctos)\n",
    "def calculate_combined_accuracy(model, generator):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for i in range(len(generator)):\n",
    "        X, y = generator[i]\n",
    "        preds = model.predict(X, verbose=0)\n",
    "        \n",
    "        num_pred = preds[0].argmax(axis=1)\n",
    "        suit_pred = preds[1].argmax(axis=1)\n",
    "        num_true = y['num_output'].argmax(axis=1)\n",
    "        suit_true = y['suit_output'].argmax(axis=1)\n",
    "        \n",
    "        # Ambos deben ser correctos\n",
    "        correct += np.sum((num_pred == num_true) & (suit_pred == suit_true))\n",
    "        total += len(X)\n",
    "    \n",
    "    return correct / total\n",
    "\n",
    "combined_acc = calculate_combined_accuracy(best_model, test_gen)\n",
    "print(f'\\nAccuracy combinada (carta completa): {combined_acc:.4f}')\n",
    "print(f'Equivalente a 52 clases: {combined_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7221944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar predicciones\n",
    "X_test, y_test = test_gen[0]\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "fig, axes = plt.subplots(3, 5, figsize=(18, 10))\n",
    "for i in range(15):\n",
    "    ax = axes[i//5, i%5]\n",
    "    ax.imshow(X_test[i])\n",
    "    \n",
    "    # True labels\n",
    "    true_num = NUM_NAMES[y_test['num_output'][i].argmax()]\n",
    "    true_suit = SUIT_NAMES[y_test['suit_output'][i].argmax()]\n",
    "    \n",
    "    # Predicted labels\n",
    "    pred_num = NUM_NAMES[predictions[0][i].argmax()]\n",
    "    pred_suit = SUIT_NAMES[predictions[1][i].argmax()]\n",
    "    \n",
    "    correct = (true_num == pred_num) and (true_suit == pred_suit)\n",
    "    color = 'green' if correct else 'red'\n",
    "    \n",
    "    ax.set_title(f'True: {true_num} of {true_suit}\\nPred: {pred_num} of {pred_suit}', \n",
    "                 color=color, fontsize=9)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(path_results, name + '_predictions.png'), dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110faf72",
   "metadata": {},
   "source": [
    "## Confusion Matrices por salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabedce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "# Recopilar todas las predicciones\n",
    "all_num_true, all_num_pred = [], []\n",
    "all_suit_true, all_suit_pred = [], []\n",
    "\n",
    "for i in range(len(test_gen)):\n",
    "    X, y = test_gen[i]\n",
    "    preds = best_model.predict(X, verbose=0)\n",
    "    \n",
    "    all_num_true.extend(y['num_output'].argmax(axis=1))\n",
    "    all_num_pred.extend(preds[0].argmax(axis=1))\n",
    "    all_suit_true.extend(y['suit_output'].argmax(axis=1))\n",
    "    all_suit_pred.extend(preds[1].argmax(axis=1))\n",
    "\n",
    "# Convertir a nombres\n",
    "num_names_list = [NUM_NAMES[i] for i in range(NUM_CLASSES)]\n",
    "suit_names_list = [SUIT_NAMES[i] for i in range(SUIT_CLASSES)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a864b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report para Numero\n",
    "print('=== Classification Report - NUMERO ===')\n",
    "print(classification_report(\n",
    "    [NUM_NAMES[i] for i in all_num_true],\n",
    "    [NUM_NAMES[i] for i in all_num_pred],\n",
    "    target_names=num_names_list\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4056c4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report para Palo\n",
    "print('=== Classification Report - PALO ===')\n",
    "print(classification_report(\n",
    "    [SUIT_NAMES[i] for i in all_suit_true],\n",
    "    [SUIT_NAMES[i] for i in all_suit_pred],\n",
    "    target_names=suit_names_list\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45677603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Numero\n",
    "cm_num = confusion_matrix(all_num_true, all_num_pred)\n",
    "sns.heatmap(cm_num, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=num_names_list, yticklabels=num_names_list, ax=axes[0])\n",
    "axes[0].set_title('Confusion Matrix - Numero')\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "\n",
    "# Palo\n",
    "cm_suit = confusion_matrix(all_suit_true, all_suit_pred)\n",
    "sns.heatmap(cm_suit, annot=True, fmt='d', cmap='Greens',\n",
    "            xticklabels=suit_names_list, yticklabels=suit_names_list, ax=axes[1])\n",
    "axes[1].set_title('Confusion Matrix - Palo')\n",
    "axes[1].set_ylabel('Actual')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(path_results, name + '_confusion_matrices.png'), dpi=150)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
