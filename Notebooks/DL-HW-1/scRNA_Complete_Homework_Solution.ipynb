{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIy4lQuj_fir"
      },
      "source": [
        "**Assignment**\n",
        "\n",
        "1. Remember to add your name to the title of the notebook\n",
        "2. The goal is to explore models that underfit and overfit, and to deal with overfitting by using the techniques seen in class.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBEvc4qjJy_f"
      },
      "outputs": [],
      "source": [
        "# Import needed libraries\n",
        "import numpy as np\n",
        "import sys, os, pdb\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o86WWaiP_xBP"
      },
      "source": [
        "Data:\n",
        "\n",
        "Consists of the gene expression profile of several cells (coming from a patient). \n",
        "\n",
        "There is a train and a test datasets already provided to you.\n",
        "\n",
        "They are organized as a matrix of cells x genes.\n",
        "\n",
        "Given a cell, the goal is to predict the correct cell-type based on the genes' expressions for that sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cambiar el directorio de trabajo a DL-HW-1 (se asume que `os` ya fue importado en otra celda)\n",
        "target_dir = \"DL-HW-1\"\n",
        "\n",
        "if os.path.isdir(target_dir):\n",
        "    os.chdir(target_dir)\n",
        "    print(f\"Directorio cambiado a: {os.getcwd()}\")\n",
        "    print(\"Contenido del directorio:\", os.listdir('.'))\n",
        "else:\n",
        "    raise FileNotFoundError(f\"Directorio '{target_dir}' no existe. Ruta actual: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VR60DkbIL910"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "\n",
        "# Path to source batch\n",
        "train_path = \"train.pkl\"\n",
        "# Path to target batch\n",
        "test_path = \"test.pkl\"\n",
        "# Column containing cell-types\n",
        "lname = \"labels\" \n",
        "\n",
        "train_batch = pd.read_pickle(train_path)\n",
        "test_batch = pd.read_pickle(test_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRITfVcHdwNq"
      },
      "outputs": [],
      "source": [
        "train_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGI7kSRwdvV6"
      },
      "outputs": [],
      "source": [
        "# Extract the common genes so that we can use the same network for both batches\n",
        "\n",
        "common_genes = list(set(train_batch.columns).intersection(set(test_batch.columns)))\n",
        "common_genes.sort()\n",
        "train_batch = train_batch[list(common_genes)]\n",
        "test_batch = test_batch[list(common_genes)]\n",
        "\n",
        "train_mat = train_batch.drop(lname, axis=1)\n",
        "train_labels = train_batch[lname]\n",
        "\n",
        "test_mat = test_batch.drop(lname, axis=1)\n",
        "test_labels = test_batch[lname]\n",
        "\n",
        "# values are already normalized (ignore this)\n",
        "mat = train_mat.values\n",
        "mat_round = np.rint(mat)\n",
        "error = np.mean(np.abs(mat - mat_round))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pz7_aiDwn0OQ"
      },
      "outputs": [],
      "source": [
        "train_labels.unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Preguntas Teoricas (Q1-Q6)\n",
        "\n",
        "### Q1: Que tipo de problema estamos resolviendo?\n",
        "\n",
        "**Respuesta:** Problema de clasificacion multiclase. Dado el perfil de expresion genica de una celula, predecir su tipo celular.\n",
        "\n",
        "### Q2: Cual es el tamano de la entrada (numero de features)?\n",
        "\n",
        "**Respuesta:** El numero de genes comunes entre train y test (se calcula en las celdas anteriores).\n",
        "\n",
        "### Q3: Cuantas neuronas debemos tener en la ultima capa?\n",
        "\n",
        "**Respuesta:** Una neurona por cada clase (tipo celular).\n",
        "\n",
        "### Q4: Cual es la funcion de activacion mas apropiada para la ultima capa?\n",
        "\n",
        "**Respuesta:** Softmax, para obtener probabilidades que sumen 1.0.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Procesar etiquetas: convertir a enteros y one-hot\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "train_labels_int = label_encoder.fit_transform(train_labels)\n",
        "test_labels_int = label_encoder.transform(test_labels)\n",
        "\n",
        "num_classes = len(label_encoder.classes_)\n",
        "n_features = train_mat.shape[1]\n",
        "\n",
        "train_labels_onehot = to_categorical(train_labels_int, num_classes)\n",
        "test_labels_onehot = to_categorical(test_labels_int, num_classes)\n",
        "\n",
        "print(f\"Numero de clases: {num_classes}\")\n",
        "print(f\"Numero de features: {n_features}\")\n",
        "print(f\"Mapeo: {dict(zip(label_encoder.classes_, range(num_classes)))}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q5: Como se modificaron las etiquetas?\n",
        "\n",
        "**Respuesta:** Se convirtieron de textos a enteros (LabelEncoder) y luego a one-hot encoding para usar con categorical_crossentropy.\n",
        "\n",
        "### Q6: Que funcion de perdida se usara?\n",
        "\n",
        "**Respuesta:** Categorical cross-entropy, la perdida estandar para clasificacion multiclase con one-hot encoding.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Entrenamiento de Modelos\n",
        "\n",
        "Entrenaremos 3 modelos sin regularizacion:\n",
        "1. Underfit: Muy pocas capas/neuronas\n",
        "2. OK: Arquitectura razonable\n",
        "3. Overfit: Muchas capas/neuronas\n",
        "\n",
        "Luego aplicaremos regularizacion (L2, Dropout) al modelo que overfit.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preparar datos\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train_full = train_mat.values.astype('float32')\n",
        "X_test = test_mat.values.astype('float32')\n",
        "y_train_full = train_labels_onehot\n",
        "y_test = test_labels_onehot\n",
        "\n",
        "# Split train/validation\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_full, y_train_full, test_size=0.2, random_state=42,\n",
        "    stratify=np.argmax(y_train_full, axis=1)\n",
        ")\n",
        "\n",
        "print(f\"X_train: {X_train.shape}, X_val: {X_val.shape}, X_test: {X_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importar TensorFlow/Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import models, layers, regularizers\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modelo 1: Underfit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Modelo con capacidad insuficiente\n",
        "def create_underfit_model():\n",
        "    model = models.Sequential([\n",
        "        layers.Dense(16, activation='relu', input_shape=(n_features,)),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ], name='Underfit')\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model_underfit = create_underfit_model()\n",
        "model_underfit.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Entrenar modelo underfit\n",
        "history_underfit = model_underfit.fit(\n",
        "    X_train, y_train, epochs=50, batch_size=32,\n",
        "    validation_data=(X_val, y_val), verbose=0\n",
        ")\n",
        "\n",
        "print(f\"Train acc: {history_underfit.history['accuracy'][-1]:.4f}\")\n",
        "print(f\"Val acc: {history_underfit.history['val_accuracy'][-1]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modelo 2: Bien Ajustado\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Modelo con capacidad adecuada\n",
        "def create_ok_model():\n",
        "    model = models.Sequential([\n",
        "        layers.Dense(128, activation='relu', input_shape=(n_features,)),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ], name='OK')\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model_ok = create_ok_model()\n",
        "model_ok.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Entrenar modelo OK\n",
        "history_ok = model_ok.fit(\n",
        "    X_train, y_train, epochs=50, batch_size=32,\n",
        "    validation_data=(X_val, y_val), verbose=0\n",
        ")\n",
        "\n",
        "print(f\"Train acc: {history_ok.history['accuracy'][-1]:.4f}\")\n",
        "print(f\"Val acc: {history_ok.history['val_accuracy'][-1]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modelo 3: Overfit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Modelo con capacidad excesiva\n",
        "def create_overfit_model():\n",
        "    model = models.Sequential([\n",
        "        layers.Dense(512, activation='relu', input_shape=(n_features,)),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(256, activation='relu'),\n",
        "        layers.Dense(256, activation='relu'),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ], name='Overfit')\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model_overfit = create_overfit_model()\n",
        "model_overfit.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Entrenar modelo overfit\n",
        "history_overfit = model_overfit.fit(\n",
        "    X_train, y_train, epochs=50, batch_size=32,\n",
        "    validation_data=(X_val, y_val), verbose=0\n",
        ")\n",
        "\n",
        "print(f\"Train acc: {history_overfit.history['accuracy'][-1]:.4f}\")\n",
        "print(f\"Val acc: {history_overfit.history['val_accuracy'][-1]:.4f}\")\n",
        "print(f\"Gap: {history_overfit.history['accuracy'][-1] - history_overfit.history['val_accuracy'][-1]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualizacion de Resultados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Graficar curvas de entrenamiento\n",
        "def plot_history(history, title):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "    ax1.plot(history.history['loss'], label='Train')\n",
        "    ax1.plot(history.history['val_loss'], label='Val')\n",
        "    ax1.set_title(f'{title} - Loss')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    ax2.plot(history.history['accuracy'], label='Train')\n",
        "    ax2.plot(history.history['val_accuracy'], label='Val')\n",
        "    ax2.set_title(f'{title} - Accuracy')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_history(history_underfit, 'Underfit Model')\n",
        "plot_history(history_ok, 'OK Model')\n",
        "plot_history(history_overfit, 'Overfit Model')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Regularizacion\n",
        "\n",
        "Aplicaremos L2 y Dropout al modelo que overfit.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Regularizacion L2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Modelo con L2 regularization\n",
        "def create_l2_model(l2_lambda=0.001):\n",
        "    model = models.Sequential([\n",
        "        layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(l2_lambda), input_shape=(n_features,)),\n",
        "        layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(l2_lambda)),\n",
        "        layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(l2_lambda)),\n",
        "        layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(l2_lambda)),\n",
        "        layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(l2_lambda)),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ], name=f'L2_{l2_lambda}')\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model_l2 = create_l2_model(0.001)\n",
        "history_l2 = model_l2.fit(\n",
        "    X_train, y_train, epochs=50, batch_size=32,\n",
        "    validation_data=(X_val, y_val), verbose=0\n",
        ")\n",
        "\n",
        "print(f\"L2 Model - Train acc: {history_l2.history['accuracy'][-1]:.4f}\")\n",
        "print(f\"L2 Model - Val acc: {history_l2.history['val_accuracy'][-1]:.4f}\")\n",
        "print(f\"L2 Model - Gap: {history_l2.history['accuracy'][-1] - history_l2.history['val_accuracy'][-1]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Regularizacion Dropout\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Modelo con Dropout\n",
        "def create_dropout_model(dropout_rate=0.5):\n",
        "    model = models.Sequential([\n",
        "        layers.Dense(512, activation='relu', input_shape=(n_features,)),\n",
        "        layers.Dropout(dropout_rate),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dropout(dropout_rate),\n",
        "        layers.Dense(256, activation='relu'),\n",
        "        layers.Dropout(dropout_rate),\n",
        "        layers.Dense(256, activation='relu'),\n",
        "        layers.Dropout(dropout_rate),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dropout(dropout_rate),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ], name=f'Dropout_{dropout_rate}')\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model_dropout = create_dropout_model(0.5)\n",
        "history_dropout = model_dropout.fit(\n",
        "    X_train, y_train, epochs=50, batch_size=32,\n",
        "    validation_data=(X_val, y_val), verbose=0\n",
        ")\n",
        "\n",
        "print(f\"Dropout Model - Train acc: {history_dropout.history['accuracy'][-1]:.4f}\")\n",
        "print(f\"Dropout Model - Val acc: {history_dropout.history['val_accuracy'][-1]:.4f}\")\n",
        "print(f\"Dropout Model - Gap: {history_dropout.history['accuracy'][-1] - history_dropout.history['val_accuracy'][-1]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comparacion de regularizacion\n",
        "plot_history(history_l2, 'L2 Regularization')\n",
        "plot_history(history_dropout, 'Dropout Regularization')\n",
        "\n",
        "print(\"\\nComparacion:\")\n",
        "print(f\"Sin reg - Gap: {history_overfit.history['accuracy'][-1] - history_overfit.history['val_accuracy'][-1]:.4f}\")\n",
        "print(f\"L2      - Gap: {history_l2.history['accuracy'][-1] - history_l2.history['val_accuracy'][-1]:.4f}\")\n",
        "print(f\"Dropout - Gap: {history_dropout.history['accuracy'][-1] - history_dropout.history['val_accuracy'][-1]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Evaluacion en Test Set\n",
        "\n",
        "Evaluaremos el mejor modelo en el conjunto de test.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluar en test set (usamos el modelo OK como ejemplo)\n",
        "test_loss, test_acc = model_ok.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Matriz de confusion\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "y_pred = model_ok.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_test_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "cm = confusion_matrix(y_test_classes, y_pred_classes)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=label_encoder.classes_,\n",
        "            yticklabels=label_encoder.classes_)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_classes, y_pred_classes, \n",
        "                          target_names=label_encoder.classes_))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Conclusiones\n",
        "\n",
        "### Resumen de Resultados:\n",
        "\n",
        "1. **Modelo Underfit**: Tiene muy poca capacidad (solo 16 neuronas en una capa). No puede aprender los patrones complejos. Bajo rendimiento en train y validation.\n",
        "\n",
        "2. **Modelo OK**: Arquitectura balanceada (128-64-32 neuronas en 3 capas). Buen rendimiento en train y validation con gap razonable.\n",
        "\n",
        "3. **Modelo Overfit**: Muchas capas y neuronas (512-512-256-256-128). Alto rendimiento en train pero bajo en validation. Gran gap indica overfitting.\n",
        "\n",
        "### Regularizacion:\n",
        "\n",
        "- **L2**: Penaliza pesos grandes. Reduce overfitting forzando pesos pequenos.\n",
        "- **Dropout**: Desactiva neuronas aleatoriamente. Evita co-adaptacion y mejora generalizacion.\n",
        "\n",
        "Ambas tecnicas reducen el gap train-validation, mejorando la capacidad de generalizacion del modelo.\n",
        "\n",
        "### Mejoras Futuras:\n",
        "\n",
        "- Mas datos de entrenamiento\n",
        "- Feature selection/engineering\n",
        "- Probar otras arquitecturas (ResNet, attention)\n",
        "- Ensemble de modelos\n",
        "- Data augmentation especifica para genomica\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}